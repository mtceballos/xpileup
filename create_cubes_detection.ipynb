{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b70bfb3",
   "metadata": {},
   "source": [
    "# Analysis of detection algorithms for SIRENA\n",
    "\n",
    "Check results of possible input parameters\n",
    "\n",
    "* samplesUp (fixed): number of consecutive samples in the derivative above the threshold\n",
    "* samplesDown (fixed): number of consecutive samples in the derivative below the threshold to start triggering again\n",
    "* threshold (fixed): value to be crossed by the derivative\n",
    "\n",
    "* window: size (samples) of the window to calculate average derivative and do a subtraction   \n",
    "  Ex. window = 3  :  \n",
    "  ```\n",
    "  deriv[i] => deriv[i] - mean(deriv[i-1], deriv[i-2], deriv[i-3])\n",
    "  ```\n",
    "\n",
    "* offset: offset (samples) of the subtracting window   \n",
    "  Ex. window = 3 && offset = 2  :   \n",
    "  ```\n",
    "  deriv[i] => deriv[i] - mean(deriv[i-3], deriv[i-4], deriv[i-5])\n",
    "  ```\n",
    "\n",
    "## Procedure \n",
    "\n",
    "1) (*external*) XIFUSIM files with 100 pairs of pulses are simulated:    \n",
    "```python\n",
    "    Eprimary = [0.2, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9 , 10, 11 ,12]    \n",
    "    Esecondary = [0.2, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9 , 10, 11 ,12]    \n",
    "    Separations = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50,60,70,80,90,100,110,126]    \n",
    "```\n",
    "2) (*external*) SIRENA reconstructed (xifusim) files using combinations of window and offset:    \n",
    "```python\n",
    "    samplesUp = 3\n",
    "    samplesDown = 2   \n",
    "    threshold = 6\n",
    "    window = [0, 1, 2, 3, 4, 5, 6, 10, 15, 20]\n",
    "    offset = [0, 1, 2, 3, 4, 5, 6]\n",
    "    The combination window=0 & offset=0 corresponds to the traditional method (no derivative subtraction)   \n",
    "```\n",
    "3) (*external*) For each window/offset combination a pickle object (file) is created with the following information:   \n",
    "```python\n",
    "    | separation | energy1 | energy2 | window | offset | ndetected | nfake |\n",
    "    \n",
    "```\n",
    "4) Analysis in this notebook:    \n",
    "  \n",
    "   a) (Optional): calculate **weights** for separations according to (Poiss) mathematical prob distribution   \n",
    "   b) (Optional): calculate **probability distribution** of energies in the pairs according to a source spectral model    \n",
    "   c) (Optional): create a source probability cube to correct the simulations cube that has a uniform distribution of energies and separations   \n",
    "   d) For each window/offset:\n",
    "    * Read pickle file   \n",
    "    * (Optionally) create a FITS data cube of number of detected photons: AXIS1-ENERGY1, AXIS2-ENERGY2, AXIS3-separations   \n",
    "    * (Optionally) multiply the simulated cube by the probability cube to work with realistic simulations\n",
    "    * Save the fraction of detections: numpy[separations, window, offset]  (**Warning**: indexing in numpy and FITS is reversed)  \n",
    "    * Plot an image of E2 vs E1 for a given separation (data cube slice)   \n",
    "\n",
    "   e) Create a mosaic of images with all the windows and offsets    \n",
    "   f) Write a FITS cube with fraction of detections: AXIS1-window, AXIS2-offset, AXIS3-separations    \n",
    "   d) Collapse the cube in energies (sum of all fractions) and separations (mean value): plot image of fraction of detections (offset vs window)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e4b07",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "> **NOTE**:   \n",
    "> to convert this notebook into a Python script (for Slurm), just \"*Export as*\" -> Python and comment the line: `%matplotlib widget`\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7528171",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e29022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "# import for pickling\n",
    "import pickle\n",
    "from astropy import table\n",
    "from astropy.io import fits\n",
    "import auxiliary as aux\n",
    "import matplotlib.colors as mcolors\n",
    "import mplcursors\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "#%matplotlib qt\n",
    "#%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed8d28",
   "metadata": {},
   "source": [
    "## Running Jupyter or Python script?   \n",
    "* It tries to call get_ipython() (only available in IPython environments, like Jupyter).   \n",
    "* If the shell class name is \"ZMQInteractiveShell\", it confirms that you're in a Jupyter notebook (or JupyterLab).      \n",
    "* If it's a regular Python interpreter, the function returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter handling\n",
    "def get_parameters():\n",
    "    \"\"\"\n",
    "    Get parameters for pairs detection analysis.\n",
    "    If running in a Jupyter Notebook, use default parameters.\n",
    "    If running as a script (e.g., SLURM), parse command line arguments.\n",
    "    \"\"\"\n",
    "    global th, sUp, sDown, windows, offsets, relevant_separations, xifu_config, create_cubes, sep_for_plot_mosaic\n",
    "\n",
    "    # Check if running in a Jupyter Notebook or as a script \n",
    "    if aux.is_notebook():\n",
    "        # Default parameters for interactive use\n",
    "        print(\"Running in notebook mode for pairs detection analysis\")\n",
    "        return {\n",
    "            \"threshold\": 6.0, # threshold for detection\n",
    "            \"samplesUp\": 3,  # samples up for detection\n",
    "            \"samplesDown\": 2, # samples down for detection\n",
    "            \"windows\": [0,1, 2, 3, 4, 5, 6, 10, 15, 20], # subtraction derivative window for detection\n",
    "            #\"windows\": [1, 2, 3, 4, 5, 6], # subtraction derivative window for detection\n",
    "            \"offsets\": [0,1, 2, 3, 4, 5, 6],  # offset for subtraction window\n",
    "            #\"offsets\": [1, 2, 3, 4, 5, 6],  # offset for subtraction window\n",
    "            \"relevant_separations\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50,60,70,80,90,100,110,126], # relevant separations for the analysis\n",
    "            \"config_version\": 'v5_20250621',  # XIFU configuration\n",
    "            \"sep_for_plot_mosaic\": 8, # samples separation for plotting the mosaic of slices of the data cube (if negative, no plotting)\n",
    "            \"win_collapsed_cube\": [0,1,2,3,4,5,6], # window sizes for the collapsed cube\n",
    "            \"use_sim_source\": True,  # use simulated source for cube correction\n",
    "            \"sim_source_flux\": 0.5,  # flux of a simulated source (to get the \"real\" distribution of pairs\n",
    "            \"verbose\": 1  # verbosity flag\n",
    "        }\n",
    "    else:\n",
    "        # Parameters from command line (e.g., for SLURM)\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description='Execute the python script for pairs detection analysis',\n",
    "            prog='execute_create_cubes.py')\n",
    "        parser.add_argument('--windows', required=False, type=int,\n",
    "                            nargs='*', default=[0, 1, 2, 3, 4, 5, 10, 15, 20],\n",
    "                            help='Subtraction derivative window for detection')\n",
    "        parser.add_argument('--offsets', required=False, type=int,\n",
    "                            nargs='*', default=[0, 1, 2, 3, 4, 5],\n",
    "                            help='Offset for subtraction window')           \n",
    "        parser.add_argument('--threshold', required=False, type=float, default=0.5,\n",
    "                            help='Threshold for detection')\n",
    "        parser.add_argument('--samplesUp', required=False, type=int, default=2,\n",
    "                            help='Samples up for detection')\n",
    "        parser.add_argument('--samplesDown', required=False, type=int, default=2,\n",
    "                            help='Samples down for detection')\n",
    "        parser.add_argument('--config_version', required=False, type=str, default='v5_20250621',\n",
    "                            help='XIFU configuration version')\n",
    "        parser.add_argument('--relevant_separations', required=False, type=int,\n",
    "                            nargs='*', default=[8, 20, 50, 126, 317, 797],\n",
    "                            help='Relevant separations for the analysis')\n",
    "        parser.add_argument('--sep_for_plot_mosaic', required=False, type=int, default=-1,\n",
    "                            help='Samples separation for plotting the mosaic of slices of the data cube (if negative, no plotting)')\n",
    "        parser.add_argument('--win_collapsed_cube', required=False, type=int,\n",
    "                            nargs='*', default=[0, 1, 2, 3, 4, 5, 6],\n",
    "                            help='Window sizes for the collapsed cube')\n",
    "        parser.add_argument('--sim_source_flux', required=False, type=float, default=0.,\n",
    "                            help='Flux of a simulated source (to get the \"real\" distribution of pairs)')\n",
    "        parser.add_argument('--use_sim_source', action='store_true',\n",
    "                            help='Use simulated source for cube correction')\n",
    "        parser.add_argument('--verbose', required=False, type=int, default=0,\n",
    "                            help='Verbosity flag (0: no output, 1: some output, 2: detailed output)')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "      \n",
    "        return vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a009c7",
   "metadata": {},
   "source": [
    "## Get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_parameters()\n",
    "th = params['threshold']\n",
    "sUp = params['samplesUp']\n",
    "sDown = params['samplesDown']\n",
    "windows = params['windows']\n",
    "offsets = params['offsets']\n",
    "xifu_config = params['config_version']\n",
    "relevant_separations = params['relevant_separations']\n",
    "sep_for_plot_mosaic = params['sep_for_plot_mosaic']\n",
    "windows_for_collapsed_cube = params['win_collapsed_cube'] \n",
    "sim_source_flux = params['sim_source_flux']\n",
    "use_sim_source = params['use_sim_source']\n",
    "aux.verbose = params['verbose']\n",
    "print(f\"Parameters for pairs detection analysis: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdff1c",
   "metadata": {},
   "source": [
    "### Secondary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 130210 # Hz\n",
    "min_detected = 100\n",
    "max_detected = 200\n",
    "energy_bin_centers = [0.2, 0.5, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12.]\n",
    "energy_bin_edges = [energy_bin_centers[0] - (energy_bin_centers[1] - energy_bin_centers[0]) / 2] + \\\n",
    "             [(energy_bin_centers[i] + energy_bin_centers[i+1]) / 2 for i in range(len(energy_bin_centers) - 1)] + \\\n",
    "             [energy_bin_centers[-1] + (energy_bin_centers[-1] - energy_bin_centers[-2]) / 2]\n",
    "seps_bin_centers = relevant_separations\n",
    "seps_bin_edges = [seps_bin_centers[0] - (seps_bin_centers[1] - seps_bin_centers[0]) / 2] + \\\n",
    "             [(seps_bin_centers[i] + seps_bin_centers[i+1]) / 2 for i in range(len(seps_bin_centers) - 1)] + \\\n",
    "             [seps_bin_centers[-1] + (seps_bin_centers[-1] - seps_bin_centers[-2]) / 2]\n",
    "            \n",
    "EURECA_dir = \"/dataj6/ceballos/INSTRUMEN/EURECA/\"\n",
    "analysis_dir = f\"{EURECA_dir}/TN350_detection/2024_revision/\"\n",
    "pickles_dir = f\"{EURECA_dir}/ERESOL/CEASaclay/July2025_v5_v20250621_offsetWindow/\"\n",
    "old_separations = [8, 20, 50, 126]  # default meaningful separations for window=0,10,15,20\n",
    "old_windows = [0, 10, 15, 20]  # old windows for the analysis\n",
    "nsimulated_1e1_1e2_1sep = 200 # number of simulated events for each separation an a given combination of E1 and E2\n",
    "nsimulated_1sep = len(energy_bin_centers) * len(energy_bin_centers) * nsimulated_1e1_1e2_1sep  # total number of simulated events for a given separation\n",
    "nsims = 100  # number of simulations to consider\n",
    "ntotal_pixels = 1504 # total number of XIFU pixels\n",
    "ntop_pixels = 10  # number of top pixels to consider for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15180a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_hover(event, text_obj, data, bin_edges, ax, fig):\n",
    "    if event.inaxes != ax:\n",
    "        text_obj.set_text('')\n",
    "        fig.canvas.draw_idle()\n",
    "        return\n",
    "        \n",
    "    if event.xdata is None or event.ydata is None:\n",
    "        text_obj.set_text('')\n",
    "        fig.canvas.draw_idle()\n",
    "        return\n",
    "        \n",
    "    x, y = event.xdata, event.ydata\n",
    "    \n",
    "    # Find the bin index for x and y using np.searchsorted\n",
    "    col = np.searchsorted(bin_edges, x) - 1\n",
    "    row = np.searchsorted(bin_edges, y) - 1\n",
    "    \n",
    "    # Check bounds\n",
    "    if 0 <= row < data.shape[0] and 0 <= col < data.shape[1]:\n",
    "        val = data[row, col]\n",
    "        x_center = bin_edges[col]\n",
    "        y_center = bin_edges[row]\n",
    "        text_obj.set_text(f'E1: {x_center:.1f} keV\\nE2: {y_center:.1f} keV\\nProb: {val:.2e}')\n",
    "    else:\n",
    "        text_obj.set_text('Outside bounds')\n",
    "    \n",
    "    fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af8f05",
   "metadata": {},
   "source": [
    "## Get time distribution weights (using Poisson stats)   \n",
    "This will be used to give different weights to the pairs separations in the test data cube    \n",
    "We will check that these weights are fully similar for all the pixels (at the end we may want to calculate only the weights for the most populated pixels and all the simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d75c1",
   "metadata": {},
   "source": [
    "* Get the list of the most populated pixels (Use only simulation 1)\n",
    "* calculate separation probability (& weights): $prob = exp(-\\lambda*s1) - exp(-\\lambda*s2)$     \n",
    "* plot weights for different countrates in sim_1     \n",
    "* as they are similar, use only the count rate of the pixel with more counts for the rest of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if use_sim_source:\n",
    "    # Use \"real\" source simulations to get the impacting photons\n",
    "    sim_source_dir = f\"{analysis_dir}/{xifu_config}/flux{sim_source_flux:.2f}mcrab\"\n",
    "    #initialize to 0 1D numpy array for counts in most populated pixels\n",
    "    counts_pixel = np.zeros(ntotal_pixels)\n",
    "\n",
    "    # get the top most populated pixels in simulation 1\n",
    "    isim = 1\n",
    "    # get list of impacted pixels\n",
    "    sim1_piximpact_files = glob.glob(f\"{sim_source_dir}/sim_{isim}/crab_flux*_Emin2_Emax10_exp*_RA0.0_Dec0.0_*_*pixel*_piximpact.fits\")\n",
    "    if len(sim1_piximpact_files) == 0:\n",
    "        raise ValueError(f\"No pixel impact files found in {sim_source_dir}/sim_{isim}/ for simulation {isim}.\")\n",
    "    \n",
    "    # sort files by size (descending order) and get the top most large files\n",
    "    piximpact_files = sorted(sim1_piximpact_files, key=os.path.getsize, reverse=True)[:ntop_pixels]\n",
    "    # get pixel numbers from filenames\n",
    "    top_pixel_nums = [int(f.split('_')[-2].replace('pixel', '')) for f in piximpact_files]\n",
    "    \n",
    "    for i, piximpact_file in enumerate(piximpact_files):\n",
    "        pixel_num = top_pixel_nums[i]\n",
    "        piximpact_file = piximpact_files[i]\n",
    "        print(f\"Reading impact file #{i+1}/{len(piximpact_files)}: {piximpact_file}\")\n",
    "        # get exposure time from the filename\n",
    "        exposure_time = float(piximpact_file.split('_exp')[1].split('_')[0])\n",
    "        # read number of events in the pixel from keyword NAXIS2\n",
    "        with fits.open(piximpact_file) as hdul:\n",
    "            counts_pixel[pixel_num-1] = hdul[1].header['NAXIS2']\n",
    "\n",
    "    print(\"\\nTop populated pixels (by number of counts):\")\n",
    "    for pixel_num in top_pixel_nums:\n",
    "        ipixel_num = pixel_num - 1\n",
    "        aux.vprint(f\"Counts for pixel {pixel_num}: {counts_pixel[ipixel_num]}\")\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a193310",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # Use \"real\" source simulations to get the impacting photons\n",
    "    sim_source_dir = f\"{analysis_dir}/{xifu_config}/flux{sim_source_flux:.2f}mcrab\"\n",
    "    top_count_rate_file = f\"{sim_source_dir}/top_count_rate_sample.pkl\"\n",
    "    if os.path.exists(top_count_rate_file):\n",
    "        aux.vprint(f\"Loading top count rates table from {top_count_rate_file}\")\n",
    "        with open(top_count_rate_file, 'rb') as f:\n",
    "            top_table = pickle.load(f)\n",
    "            top_pixel_nums = top_table['pixel_number']\n",
    "            top_ctrate_sam = top_table['count_rate_sample']\n",
    "    else:\n",
    "        print(f\"Top count rates table {top_count_rate_file} does not exist. Will create it.\")\n",
    "\n",
    "        #initialize to 0 1D numpy array for counts in the most populated pixels\n",
    "        counts_pixel = np.zeros(ntotal_pixels)\n",
    "\n",
    "        # get the top most populated pixels in simulation 1\n",
    "        # =================================================\n",
    "        isim = 1\n",
    "        # get list of impacted pixels\n",
    "        sim1_piximpact_files = glob.glob(f\"{sim_source_dir}/sim_{isim}/crab_flux*_Emin2_Emax10_exp*_RA0.0_Dec0.0_*_*pixel*_piximpact.fits\")\n",
    "        if len(sim1_piximpact_files) == 0:\n",
    "            raise ValueError(f\"No pixel impact files found in {sim_source_dir}/sim_{isim}/ for simulation {isim}.\")\n",
    "        \n",
    "        # sort files by size (descending order) and get the top most large files\n",
    "        piximpact_files = sorted(sim1_piximpact_files, key=os.path.getsize, reverse=True)[:ntop_pixels]\n",
    "        # get pixel numbers from filenames\n",
    "        top_pixel_nums = [int(f.split('_')[-2].replace('pixel', '')) for f in piximpact_files]\n",
    "        \n",
    "        for i, piximpact_file in enumerate(piximpact_files):\n",
    "            pixel_num = top_pixel_nums[i]\n",
    "            piximpact_file = piximpact_files[i]\n",
    "            print(f\"Reading impact file #{i+1}/{len(piximpact_files)}: {piximpact_file}\")\n",
    "            # get exposure time from the filename\n",
    "            exposure_time = float(piximpact_file.split('_exp')[1].split('_')[0])\n",
    "            # read number of events in the pixel from keyword NAXIS2\n",
    "            with fits.open(piximpact_file) as hdul:\n",
    "                counts_pixel[pixel_num-1] = hdul[1].header['NAXIS2']\n",
    "\n",
    "        print(\"\\nTop populated pixels (by number of counts):\")\n",
    "        for pixel_num in top_pixel_nums:\n",
    "            ipixel_num = pixel_num - 1\n",
    "            aux.vprint(f\"Counts for pixel {pixel_num}: {counts_pixel[ipixel_num]}\")\n",
    "\n",
    "\n",
    "        # get the mean count rate (using all simulations) for the top most populated pixels\n",
    "        # =================================================\n",
    "        # initialize to 0 2D numpy array for counts in the most populated pixels\n",
    "        top_counts = np.zeros((ntop_pixels, nsims))\n",
    "        top_ctrate_sam = np.zeros(ntop_pixels)  # mean count rate per sample\n",
    "\n",
    "        for ipix in range(len(top_pixel_nums)):\n",
    "            pixel_num = top_pixel_nums[ipix]\n",
    "            aux.vprint(f\"Processing pixel {pixel_num}\")\n",
    "            # initialize counts for the impacted pixels\n",
    "            for i in range(nsims):\n",
    "                isim = i + 1\n",
    "                piximpact_file = glob.glob(f\"{sim_source_dir}/sim_{isim}/crab_flux*_Emin2_Emax10_exp*_RA0.0_Dec0.0_*_*pixel{pixel_num}_piximpact.fits\")\n",
    "                if len(piximpact_file) > 1:\n",
    "                    raise ValueError(f\"Multiple pixel impact files found for pixel {pixel_num} in simulation {isim}: {piximpact_file}\")\n",
    "                elif len(piximpact_file) == 1:\n",
    "                    piximpact_file = piximpact_file[0]\n",
    "                    # read number of events in the pixel from keyword NAXIS2\n",
    "                    with fits.open(piximpact_file) as hdul:\n",
    "                        top_counts[ipix, i] = hdul[1].header['NAXIS2']\n",
    "\n",
    "        # calculate the mean count rate (per sample) for each pixel\n",
    "        top_ctrate_sam = np.mean(top_counts, axis=1) / (exposure_time * sampling_rate)  # mean count rate per pixel\n",
    "        # create a table with pixel_number, count_rate\n",
    "        top_table = table.Table()\n",
    "        top_table['pixel_number'] = top_pixel_nums\n",
    "        top_table['count_rate_sample'] = top_ctrate_sam\n",
    "        # save the top count rate to a file\n",
    "        with open(top_count_rate_file, 'wb') as f:\n",
    "            pickle.dump(top_table, f)\n",
    "        aux.vprint(f\"Top count rates table saved to {top_count_rate_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # calculate the probability of each separation for the top most populated pixels\n",
    "    # Probability of each separation (see Nico's notebook)\n",
    "    # prob = exp(-lambda*s1) - exp(-lambda*s2)\n",
    "    # lambda = count_rate / sampling_rate\n",
    "    # s1: left border of the separation bin\n",
    "    # s2: right border of the separation bin\n",
    "    prob_sep = np.zeros((ntop_pixels, len(relevant_separations)))\n",
    "    weights_sep = np.zeros((ntop_pixels, len(relevant_separations)))\n",
    "    for ipix in range(ntop_pixels):\n",
    "        # calculate the probability of each separation for the pixel\n",
    "        for isep, sep in enumerate(relevant_separations):\n",
    "            prob_sep[ipix, isep] = np.exp(-top_ctrate_sam[ipix] * (sep-0.5)) - np.exp(-top_ctrate_sam[ipix] * (sep + 0.5))\n",
    "        # get the weights for the separations (for each pixel)\n",
    "        weights_sep[ipix, :] = prob_sep[ipix, :] / np.mean(prob_sep[ipix, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability of each separation for the 10 top most populated pixels\n",
    "if use_sim_source:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for ipix in range(ntop_pixels):\n",
    "        pixel_num = top_pixel_nums[ipix]\n",
    "        ctrate_in_pixel = top_ctrate_sam[ipix] * sampling_rate\n",
    "        ax.plot(relevant_separations, weights_sep[ipix, :], label=f\"Pixel {pixel_num} ({ctrate_in_pixel:.1f} ct/s)\", marker='o')\n",
    "        # break # uncomment to plot only the first pixel\n",
    "    ax.set_xlabel(\"Separation (samples)\")\n",
    "    ax.set_ylabel(\"Weight\")\n",
    "    ax.set_title(f\"Weights of each separation for the {ntop_pixels} top most populated pixels\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9af37b",
   "metadata": {},
   "source": [
    "## Get energy distribution probability (using the simulated source spectrum)\n",
    "\n",
    "Once we have seen that the probability curves (weights of different separations) are essentially identical for all the main pixels (i.e. for small separations, where problems can arise, it does not change a lot with the count rate), we'll use only the most impacted pixel to proceed with the analysis.\n",
    "\n",
    "Use the list of energies of the photons in the pixel to create an energy distribution map -> probability distribution of energies\n",
    "\n",
    "We will use this probability map to correct the number of pairs simulated in the test cube according to the input spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59702558",
   "metadata": {},
   "source": [
    "### Get distribution of energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # take the first pixel as the main pixel (more counts)\n",
    "    main_pixel_num = top_pixel_nums[0]  # first pixel in the list of top pixels\n",
    "    main_pixel_energies_file = f\"{sim_source_dir}/pixel{main_pixel_num}_energies.pkl\"\n",
    "    if os.path.exists(main_pixel_energies_file):\n",
    "        aux.vprint(f\"Loading main pixel energies from {main_pixel_energies_file}\")\n",
    "        with open(main_pixel_energies_file, 'rb') as f:\n",
    "            E1E2_energies = pickle.load(f)\n",
    "    else:\n",
    "        # take the last pixel as the main pixel (more counts)\n",
    "        aux.vprint(\"Main pixel energies file does not exist. Will create it.\")\n",
    "        main_pixel_num = top_pixel_nums[0]  # first pixel in the list of top pixels\n",
    "        #index_main_pixel = main_pixel_num - 1  # index of the main pixel in the counts array\n",
    "        main_pixel_counts = top_counts[0, :].sum()  # total counts in the main pixel\n",
    "        aux.vprint(f\"Using main pixel for analysis: {main_pixel_num} with {main_pixel_counts} counts\")\n",
    "\n",
    "        all_energies = []  # to store energies of all simulations\n",
    "\n",
    "        # Get a \"real\" spectrum of the simulated source in one (maximum) pixel: read the energies of the photons in all simulations\n",
    "        aux.vprint(f\"Simulated source flux: {sim_source_flux} mCrab\")\n",
    "        for isim in range(1,nsims+1):\n",
    "            piximpact_file = glob.glob(f\"{sim_source_dir}/sim_{isim}/crab_flux*_*_pixel{main_pixel_num}_piximpact.fits\")\n",
    "            if len(piximpact_file) > 1:\n",
    "                raise ValueError(f\"Multiple piximpact files for pixel {main_pixel_num} found for simulated source: {piximpact_file}\")\n",
    "            elif len(piximpact_file) == 0:\n",
    "                raise FileNotFoundError(f\"No piximpact file found for simulated source in pixel {main_pixel_num} at {sim_source_dir}/sim_{isim}/\")\n",
    "            piximpact_file = piximpact_file[0]  # take the file found\n",
    "            #print(f\"Reading piximpact file for simulated source: {piximpact_file}\")\n",
    "            \n",
    "            with fits.open(piximpact_file) as hdul:\n",
    "                aux.vprint(f\"Reading piximpact file for simulation {isim} \\r\", end=\"\")\n",
    "                # Assuming the first HDU contains the data\n",
    "                data = hdul[0].data\n",
    "                # read ENERGY of photons\n",
    "                energies = hdul[1].data['ENERGY']\n",
    "                all_energies.append(energies)\n",
    "        # concatenate all energies from different simulations\n",
    "        E1E2_energies = np.concatenate(all_energies)  # concatenate all energies from different simulations\n",
    "        if len(E1E2_energies) != main_pixel_counts:\n",
    "            raise ValueError(f\"Inconsistent number of energies found in the simulated source for pixel {main_pixel_num}.\")\n",
    "        # save the energies of the photons in the main pixel\n",
    "        aux.vprint(f\"Saving main pixel energies to {main_pixel_energies_file}\")\n",
    "        with open(main_pixel_energies_file, 'wb') as f:\n",
    "            pickle.dump(E1E2_energies, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df1fcb",
   "metadata": {},
   "source": [
    "### Plot energy distribution map (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # Compute 1D histogram of energies\n",
    "    ener_hist, _ = np.histogram(E1E2_energies, bins=energy_bin_edges)\n",
    "\n",
    "    # Now create the 2D outer product to form the 2D histogram image\n",
    "    ener_hist2d = np.outer(ener_hist, ener_hist)\n",
    "\n",
    "    # Normalize to make it a probability image\n",
    "    prob_matrix = ener_hist2d / np.sum(ener_hist2d)\n",
    "    # check that the probability matrix is normalized\n",
    "    if np.sum(prob_matrix) != 1:\n",
    "        raise ValueError(\"The probability matrix is not normalized. Check the energies of the photons in the main pixel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Set the colormap and normalization\n",
    "    vmin = np.min(prob_matrix[prob_matrix > 0])  # minimum value for normalization\n",
    "    vmax = np.max(prob_matrix)  # maximum value for normalization\n",
    "    norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "    # Create the image with the specified colormap and normalization\n",
    "    X, Y = np.meshgrid(energy_bin_edges, energy_bin_edges)\n",
    "    # Create the image with the specified colormap and normalization    \n",
    "    source_mesh = ax.pcolormesh(X, Y, prob_matrix, cmap='viridis', norm=norm, shading='auto')\n",
    "    ax.set_xlabel('Energy1')\n",
    "    ax.set_ylabel('Energy2')\n",
    "    ax.set_title(f'Energies probability distribution (Crab model, flux={sim_source_flux:.2f} mCrab)')\n",
    "    fig.colorbar(source_mesh, ax=ax, label='Probability')\n",
    "    \n",
    "   # Create a text object for displaying values\n",
    "    text_obj = ax.text(0.02, 0.98, '', transform=ax.transAxes, \n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                      verticalalignment='top')\n",
    "    # Connect the hover event\n",
    "    cid = fig.canvas.mpl_connect('motion_notify_event', lambda event: on_hover(event, text_obj, prob_matrix, energy_bin_edges, ax, fig))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2649b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f'analysis_pairs/Figures/prob_energy_distrib_{sim_source_flux:.2f}mcrab.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ceb840",
   "metadata": {},
   "source": [
    "## Create the probability cube (energy*separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # multiply the prob_matrix by the prob_sep and create a cube\n",
    "    prob_cube = np.zeros((len(seps_bin_edges)-1, len(energy_bin_edges)-1, len(energy_bin_edges)-1))\n",
    "    for isep, sep in enumerate(relevant_separations):\n",
    "        prob_cube[isep:, :, :] = prob_matrix * weights_sep[0, isep]  # use the top pixel probabilities\n",
    "    # create a FITS file with the probability cube\n",
    "    prob_cube_file = f\"{sim_source_dir}/probability_cube_pixel{main_pixel_num}.fits\"\n",
    "    hdu = fits.PrimaryHDU(prob_cube)\n",
    "    hdu.header['XIFU_CFG'] = xifu_config\n",
    "    hdu.header['FLUX'] = sim_source_flux\n",
    "    hdu.header['MAXPIXEL'] = main_pixel_num\n",
    "    hdu.writeto(prob_cube_file, overwrite=True)\n",
    "    aux.vprint(f\"Probability cube shape: {prob_cube.shape} saved to {prob_cube_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a97563",
   "metadata": {},
   "source": [
    "## Analyse results for window/offset variations\n",
    "1. SIRENA reconstruction results (of uniform pairs) are saved in pickle files for each combination of window and offset    \n",
    "2. Create Data cubes (optionally save them to FITS files):    \n",
    "\n",
    " | NAXIS3(sep)   \n",
    " |        \n",
    " |____ NAXIS1(e1)     \n",
    " /         \n",
    "NAXIS2(e2)\n",
    "\n",
    "3. Save fractions of detected pulses [separation, window, offset]    \n",
    "4. Plot E2 vs E1 mosaic of images for a given separation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6938ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a numpy array 3-D (sep, offset, window) for fraction of detected pulses (opt. sim source corrected) with NaN values\n",
    "cube_e1e2collapsed = np.nan * np.full((len(relevant_separations), len(offsets), len(windows_for_collapsed_cube)), dtype=float, fill_value=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cabdad",
   "metadata": {},
   "source": [
    "### Create the CUBES (pairs, corrected, collapsed) for all win/off combinations\n",
    "\n",
    "* Avoid impossible combinations of win/off (not simulated or not meaningful)      \n",
    "* for each win/off read pickel file with number of detections   \n",
    "* create (read) cube of detections of pairs as an histogramdd (sep, e1, e2)    \n",
    "* replace with NaN those sep slices thatt have not been simulated for the current win/off    \n",
    "* if using simulated source, correct pairs cube with (e1e2 matrix probability) * (poiss weights for separations) -> save cube    \n",
    "* create collapsed cube (summing up in e1,e2 axes) to plot final map of win/off combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24084a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_separations = relevant_separations  # use the relevant separations for the analysis\n",
    "for io in range(len(offsets)):\n",
    "    for iw in range(len(windows)): \n",
    "        off = offsets[io]\n",
    "        win = windows[iw]\n",
    "        if win == 0 and off > 0:\n",
    "            # skip this combination, as it is not meaningful\n",
    "            aux.vprint(f\"    Skipping window {win} with offset {off} as it is not applicable.\")\n",
    "            continue\n",
    "        if win in old_windows and off > 5:\n",
    "            aux.vprint(f\"    Skipping window {win} with offset {off} as it is not applicable.\")\n",
    "            continue\n",
    "        aux.vprint(f\"Offset: {off}, Window: {win}\")\n",
    "        # check if cube exists\n",
    "        pairs_cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/pairs_cubes/pairs_cube_detections_win{win}_off{off}.fits\"\n",
    "        if not os.path.exists(pairs_cube_detections_iw_io_file):\n",
    "            aux.vprint(f\"    Pairs cube detections file {pairs_cube_detections_iw_io_file} does not exist. Will create it.\")\n",
    "\n",
    "            pickle_file = f'{pickles_dir}/detectedFakes_win{win}_off{off}.pkl'            \n",
    "            # read detection data from the pickle file\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                #print(data)\n",
    "                if win == 0 and off == 0:\n",
    "                    # for window=0 and offset=0, we have a different data structure\n",
    "                    data_table = table.Table(rows=data, names=('separation', 'energy1', 'energy2', 'samplesDown', 'samplesUp', 'threshold', 'ndetected', 'nfake'))\n",
    "                    data_filtered = data_table[(data_table['threshold'] == th) & (data_table['samplesUp'] == sUp) & (data_table['samplesDown'] == sDown)]\n",
    "                else:\n",
    "                    data_table = table.Table(rows=data, names=('separation', 'energy1', 'energy2', 'window', 'offset', 'ndetected', 'nfake')) \n",
    "                    data_filtered = data_table.copy()\n",
    "\n",
    "            # get only separations that are in the relevant separations list\n",
    "            data_filtered = data_filtered[np.isin(data_filtered['separation'], use_separations)]\n",
    "            #print(f\"Filtered data: {data_filtered}\")\n",
    "            \n",
    "            #look for rows where nfake > 0\n",
    "            data_filtered_nfake = data_filtered[data_filtered['nfake'] > 0]\n",
    "            if len(data_filtered_nfake) > 0:\n",
    "                aux.vprint(f\"Filtered data with nfake > 0: {data_filtered_nfake}\") \n",
    "\n",
    "            # create a data cube for each pickle file (and optionally save it to a FITS file)\n",
    "            #  (use histogramdd to create a cube of detections)\n",
    "            e1_for_pairs_cube = np.array(data_filtered['energy1'])\n",
    "            e2_for_pairs_cube = np.array(data_filtered['energy2'])\n",
    "            sep_for_pairs_cube = np.array(data_filtered['separation'])\n",
    "            detections_for_pairs_cube = np.array(data_filtered['ndetected'])\n",
    "            # NUMPY array as (sep, e2, e1) so that FITS is (e1,e2,sep)\n",
    "            coords_for_pairs_cube = np.vstack((sep_for_pairs_cube, e2_for_pairs_cube, e1_for_pairs_cube)).T  # stack the coordinates for histogramdd      \n",
    "            pairs_cube_detections_iw_io, pairs_cube_edges = np.histogramdd(coords_for_pairs_cube,\n",
    "                                                        bins=[seps_bin_edges, energy_bin_edges, energy_bin_edges], \n",
    "                                                        weights=detections_for_pairs_cube)  # create a 3D histogram with the counts\n",
    "            # BE CAREFUL: if one of the separation bins is empty, it must be because:\n",
    "            # 1. there are no detections for this separation (true 0)\n",
    "            # 2. there are no simulated pairs for this separation (false 0) -> replace 0s with NaN\n",
    "            for i, sep_edge in enumerate(seps_bin_edges[:-1]):  # iterate through bin edges except the last one\n",
    "                # check if any sep_for_pairs_cube is inside the bin\n",
    "                next_sep_edge = seps_bin_edges[i + 1]\n",
    "                if not np.any((sep_for_pairs_cube >= sep_edge) & (sep_for_pairs_cube < next_sep_edge)):\n",
    "                    # if not, replace the corresponding slice in pairs_cube_detections_iw_io with NaN\n",
    "                    pairs_cube_detections_iw_io[i, :, :] = np.nan  # replace the slice with NaN\n",
    "                    aux.vprint(f\"   In cube creation: Replacing slice for separation bin {sep_edge}-{next_sep_edge} (not present in simulations) with NaN in pairs_cube_detections_iw_io\")\n",
    "\n",
    "            # create a FITS file for the pairs cube detections\n",
    "            hdu = fits.PrimaryHDU(pairs_cube_detections_iw_io)\n",
    "            hdu.header['XIFU_CFG'] = xifu_config\n",
    "            hdu.header['FLUX'] = sim_source_flux\n",
    "            hdu.header['MAXPIXEL'] = main_pixel_num\n",
    "            hdu.writeto(pairs_cube_detections_iw_io_file, overwrite=True)\n",
    "        else:\n",
    "            aux.vprint(f\"Pairs cube detections file {pairs_cube_detections_iw_io_file} already exists. Will read it.\")\n",
    "            # read the pairs cube detections from the FITS file\n",
    "            with fits.open(pairs_cube_detections_iw_io_file) as hdul:\n",
    "                pairs_cube_detections_iw_io = hdul[0].data\n",
    "\n",
    "        # correct the PAIRS cube for the simulated source\n",
    "        if use_sim_source:            \n",
    "            # multiply this cube by the probability cube element wise\n",
    "            cube_detections_iw_io = pairs_cube_detections_iw_io * prob_cube\n",
    "            corrected_cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/corrected_cubes/corrected_cube_detections_win{win}_off{off}.fits\"\n",
    "            if not os.path.exists(corrected_cube_detections_iw_io_file):\n",
    "                aux.vprint(f\"Corrected pairs cube detections file {corrected_cube_detections_iw_io_file} does not exist. Will create it.\")\n",
    "                # save the corrected cube to\n",
    "                hdu = fits.PrimaryHDU(cube_detections_iw_io)\n",
    "                hdu.header['XIFU_CFG'] = xifu_config\n",
    "                hdu.header['FLUX'] = sim_source_flux\n",
    "                hdu.header['MAXPIXEL'] = main_pixel_num\n",
    "                hdu.writeto(corrected_cube_detections_iw_io_file, overwrite=True)\n",
    "        else:\n",
    "            #vprint(f\"Using pairs cube detections file {pairs_cube_detections_iw_io_file} as is (no correction for simulated source).\")\n",
    "            cube_detections_iw_io = pairs_cube_detections_iw_io\n",
    "            \n",
    "        # Create a collapsed cube for the pairs cube detections\n",
    "        if win in windows_for_collapsed_cube: \n",
    "            for isep, sep in enumerate(use_separations):\n",
    "                detections_slice_sep = cube_detections_iw_io[isep, :, :]  # slice for the current separation\n",
    "                prob_detections_slice_sep = detections_slice_sep / nsimulated_1e1_1e2_1sep\n",
    "                if use_sim_source:\n",
    "                    cube_e1e2collapsed[isep, io, iw] = np.sum(prob_detections_slice_sep)  # store the mean of the slice in E1,E2 in the corrected cube\n",
    "                else:\n",
    "                    cube_e1e2collapsed[isep, io, iw] = np.nanmean(prob_detections_slice_sep)\n",
    "                #print(f\"    Window: {win}, Offset: {off}, Separation: {sep}, cube_e1e2collapsed[{isep}, {io_plot}, {iw}] = {cube_e1e2collapsed[isep, io_plot, iw]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2a73c",
   "metadata": {},
   "source": [
    "### Calculate the slice for win=0 and off=0 if plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99979447",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sep_for_plot_mosaic > 0 and use_sim_source:\n",
    "    # read CUBE FITS file for the separation and win=0, offset=0\n",
    "    if use_sim_source:\n",
    "        cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/corrected_cubes/corrected_cube_detections_win0_off0.fits\"\n",
    "    else:\n",
    "        cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/pairs_cubes/pairs_cube_detections_win0_off0.fits\"\n",
    "    with fits.open(cube_detections_iw_io_file) as hdul:\n",
    "        cube_detections_iw_io = hdul[0].data\n",
    "    # create a slice for the mosaic plot\n",
    "    separation_index = np.where(np.array(use_separations) == sep_for_plot_mosaic)[0][0]  # get the index of the separation for the mosaic plot\n",
    "    slice_to_plot = cube_detections_iw_io[separation_index, :, :]\n",
    "    prob_slice_to_plot_win0_off0 = slice_to_plot / nsimulated_1e1_1e2_1sep  # use the corrected slice if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29398",
   "metadata": {},
   "source": [
    "### Plot the mosaic of differential-detection images for win/off combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc936776",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sep_for_plot_mosaic > 0:\n",
    "    e12_labs = []\n",
    "    for ilab in range(len(energy_bin_centers)):\n",
    "        if energy_bin_centers[ilab] < 1:\n",
    "            e12_labs.append(f'{energy_bin_centers[ilab]:.1f}')\n",
    "        else:\n",
    "            e12_labs.append(f'{energy_bin_centers[ilab]:.0f}')\n",
    "    \n",
    "    # vprint(\"Window    Offset    DataCube       CorrDataCube       DataSlice\")\n",
    "\n",
    "    # do not consider window=0 for the mosaic plot if use_sim_source is True\n",
    "    if use_sim_source:\n",
    "        # remove window=0 from the list of windows for the mosaic plot\n",
    "        windows_for_mosaic = [w for w in windows if w != 0]\n",
    "    else:\n",
    "        windows_for_mosaic = windows\n",
    "\n",
    "    # create a mosaic figure (with squared plots) of the same slice in different data-cubes for each window and offset\n",
    "    fig_mosaic, ax_mosaic = plt.subplots(len(offsets), len(windows_for_mosaic), figsize=(18, 12), sharex=True, sharey=True)\n",
    "    #fig_mosaic.suptitle(f'Mosaic of probability of detection (config: {xifu_config=}, {th=}, {sUp=}, {sDown=}, {sep=})', fontsize=10)\n",
    "    \n",
    "    # log scale normalization\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    if use_sim_source:\n",
    "        vmin = 1E-5 # minimum value for normalization (to avoid log(0))\n",
    "        vmax = 1\n",
    "        #norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "        norm = SymLogNorm(linthresh=5e-7, linscale=1, vmin=-1E-5, vmax=0.2, base=10)\n",
    "        neg_cmap = plt.get_cmap('Greys_r', 128)   # reversed grey for negatives\n",
    "        pos_cmap = plt.get_cmap('viridis', 128)\n",
    "        colors = np.vstack((\n",
    "            neg_cmap(np.linspace(0.4, 1, 128)),  # grey for neg\n",
    "            pos_cmap(np.linspace(0, 1, 128))     # green-yellow for pos\n",
    "        ))\n",
    "        custom_cmap = LinearSegmentedColormap.from_list('asym_div_viridis', colors)\n",
    "        fig_mosaic.suptitle(f'Differential probability of detection (wrt win=0, off=0)', fontsize=10)\n",
    "    else:\n",
    "        vmin = 1E-2\n",
    "        vmax = 1\n",
    "        norm = mcolors.Normalize(vmin=0, vmax=vmax)\n",
    "        #norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "        custom_cmap = cmap  # use the default colormap\n",
    "        fig_mosaic.suptitle(f'Probability of detection', fontsize=10)\n",
    "\n",
    "    # Plot each window and offset\n",
    "    # ===========================================\n",
    "    \n",
    "    for io in range(len(offsets)):\n",
    "        for iw in range(len(windows_for_mosaic)): \n",
    "            # get offset in inverse order for plotting reasons: mosaic plots would otherwise show the first offset at the top\n",
    "            io_plot = len(offsets) - 1 - io\n",
    "            off = offsets[io_plot]  # use the current offset for plotting\n",
    "            win = windows_for_mosaic[iw]  # use the current window for plotting (skip window=0)\n",
    "            print(f\"Offset: {off}, Window: {win}\")\n",
    "        \n",
    "            if win == 0 and off > 0:\n",
    "                # skip this combination, as it is not meaningful\n",
    "                aux.vprint(f\"    Skipping window {win} with offset {off} as it is not applicable.\")\n",
    "                continue\n",
    "        \n",
    "            if win in old_windows and off > 5:\n",
    "                print(f\"    Skipping window {win} with offset {off} as it is not applicable.\")\n",
    "                if sep_for_plot_mosaic > 0:\n",
    "                    ax_mosaic[io,iw].xaxis.set_visible(False)\n",
    "                    ax_mosaic[io,iw].yaxis.set_visible(False)\n",
    "                continue\n",
    "            \n",
    "            # read the pairs cube detections for the current window and offset\n",
    "            if use_sim_source:\n",
    "                cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/corrected_cubes/corrected_cube_detections_win{win}_off{off}.fits\"\n",
    "            else:\n",
    "                cube_detections_iw_io_file = f\"{analysis_dir}/analysis_pairs/pairs_cubes/pairs_cube_detections_win{win}_off{off}.fits\"\n",
    "            # read cube_detections_iw_io from the FITS file\n",
    "            with fits.open(cube_detections_iw_io_file) as hdul:\n",
    "                cube_detections_iw_io = hdul[0].data\n",
    "\n",
    "            # plot mosaic of slices of the data cube\n",
    "            # --------------------------------------        \n",
    "            separation_index = np.where(np.array(use_separations) == sep_for_plot_mosaic)[0][0]\n",
    "            slice_to_plot = cube_detections_iw_io[separation_index,:, :]\n",
    "            prob_slice_to_plot = slice_to_plot / nsimulated_1e1_1e2_1sep  # use the corrected slice if available\n",
    "            data = prob_slice_to_plot\n",
    "            if use_sim_source:\n",
    "                diff_prob_slice_to_plot = prob_slice_to_plot - prob_slice_to_plot_win0_off0\n",
    "                data = diff_prob_slice_to_plot  # use the difference from the first bin for plotting\n",
    "                # if there is a NaN in the slice, print a warning\n",
    "                if np.isnan(diff_prob_slice_to_plot).any():\n",
    "                    print(f\"Warning: NaN found in the slice for window {win}, offset {off}, separation {sep_for_plot_mosaic} (index {separation_index})\")\n",
    "                \n",
    "            #print(f\"       Data range: {np.min(data)}, {np.max(data)}\")\n",
    "            \n",
    "            # create the image with the specified colormap and normalization using pcolormesh (bins of different sizes) - no cursor hoovering\n",
    "            if aux.is_notebook:\n",
    "                im = ax_mosaic[io, iw].imshow(data, aspect='auto', origin='lower', cmap=custom_cmap, norm=norm, interpolation='nearest')\n",
    "                ax_mosaic[io, iw].set_xticks(np.arange(len(energy_bin_centers)))\n",
    "                ax_mosaic[io, iw].set_xticklabels(e12_labs, rotation=45, fontsize=6)\n",
    "                ax_mosaic[io, iw].set_yticks(np.arange(len(energy_bin_centers)))\n",
    "                ax_mosaic[io, iw].set_yticklabels(e12_labs, fontsize=6)\n",
    "            else:\n",
    "                X, Y = np.meshgrid(energy_bin_edges, energy_bin_edges)\n",
    "                im = ax_mosaic[io, iw].pcolormesh(X, Y, data, cmap=custom_cmap, norm=norm, shading='auto')\n",
    "                ax_mosaic[io, iw].set_xticks(energy_bin_centers[2:])  # set x-ticks only after the first two bins\n",
    "                ax_mosaic[io, iw].set_yticks(energy_bin_centers[2:])  # set y-ticks only after the first two bins\n",
    "                ax_mosaic[io, iw].set_xticklabels(e12_labs[2:], rotation=45, fontsize=6)\n",
    "                ax_mosaic[io, iw].set_yticklabels(e12_labs[2:], fontsize=6)\n",
    "            ax_mosaic[io, iw].set_title(f'Window: {win}, Offset: {off}', fontsize=7)\n",
    "            ax_mosaic[io, iw].set_xlabel('Energy primary (keV)', fontsize=7)\n",
    "            ax_mosaic[io, iw].set_ylabel('Energy secondary (keV)', fontsize=7)\n",
    "            ax_mosaic[io, iw].set_aspect('equal')\n",
    "                \n",
    "            # add color bar to each subplot\n",
    "            cbar = fig_mosaic.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=custom_cmap), ax=ax_mosaic[io, iw], fraction=0.046, pad=0.04)\n",
    "            cbar.ax.tick_params(labelsize=5)  # adjust color bar tick label size\n",
    "    # adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed22c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sep_for_plot_mosaic > 0:\n",
    "    # save the mosaic figure (png and PDF)\n",
    "    if use_sim_source:\n",
    "        aux.vprint(f\"Saving mosaic figure for separation {sep_for_plot_mosaic} with simulated source flux {sim_source_flux:.2f} mCrab\")\n",
    "        fig_mosaic.savefig(f'analysis_pairs/Figures/mosaic_prob_detected_events_slices_sep{sep_for_plot_mosaic}_windows_offsets_{sim_source_flux:.2f}mcrab.png', dpi=300, bbox_inches='tight')\n",
    "        #fig_mosaic.savefig(f'analysis_pairs/Figures/mosaic_prob_detected_events_slices_sep{sep_for_plot_mosaic}_windows_offsets_{sim_source_flux:.2f}mcrab.pdf', dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        aux.vprint(f\"Saving mosaic figure for separation {sep_for_plot_mosaic} without simulated source flux\")\n",
    "        fig_mosaic.savefig(f'analysis_pairs/Figures/mosaic_prob_detected_events_slices_sep{sep_for_plot_mosaic}_windows_offsets.png', dpi=300, bbox_inches='tight')\n",
    "        #fig_mosaic.savefig(f'analysis_pairs/Figures/mosaic_prob_detected_events_slices_sep{sep_for_plot_mosaic}_windows_offsets.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131c18c",
   "metadata": {},
   "source": [
    "## Collapse fraction_detected_pulses cube\n",
    "\n",
    "1. Take mean value along separations axis   \n",
    "2. Plot collapsed image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e426d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print the shape of the corrected cube -> should be (separations, offsets, windows)\n",
    "print(f\"cube_e1e2collapsed: {cube_e1e2collapsed.shape}\")\n",
    "# collase cube in axis 0 to get the mean of detected fractions along the separations and plot image (account for nans)\n",
    "cube_collapsed = np.nanmean(cube_e1e2collapsed, axis=0)  # collapse the cube along the separations axis\n",
    "print(f\"Collapsed cube shape: {cube_collapsed.shape}\")\n",
    "vmin = np.nanmin(cube_collapsed)\n",
    "vmax = np.nanmax(cube_collapsed)*1.01\n",
    "cmap_collapsed = plt.get_cmap('viridis').copy()  # colormap for the collapsed cube\n",
    "cmap_collapsed.set_bad(color='white')  # set NaN values to white in the colormap\n",
    "\n",
    "# create a new figure for the collapsed cube\n",
    "fig_collapsed, ax_collapsed = plt.subplots(figsize=(8, 6))\n",
    "# create a normalization for the color map\n",
    "norm_collapsed = mcolors.Normalize()\n",
    "if use_sim_source:\n",
    "    # use a two-slope normalization for the collapsed cube\n",
    "    norm_collapsed = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0.88, vmax=vmax)\n",
    "else:\n",
    "    # use a linear normalization for the collapsed cube\n",
    "    norm_collapsed = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "# mask invalid values in the collapsed cube to avoid errors while hoovering with the mouse\n",
    "masked_cube_collapsed = np.ma.masked_invalid(cube_collapsed)\n",
    "# plot the collapsed cube\n",
    "im_collapsed = ax_collapsed.imshow(masked_cube_collapsed, origin='lower', aspect='auto', cmap=cmap_collapsed, norm=norm_collapsed, interpolation='nearest')\n",
    "#ax_collapsed.set_title(f'Collapsed Fraction of detected (config: {xifu_config=}, {th=}, {sUp=}, {sDown=})', fontsize=10)\n",
    "ax_collapsed.set_title(f'Collapsed Fraction of detected pulses', fontsize=10)\n",
    "ax_collapsed.set_ylabel('Offset (samples)')\n",
    "ax_collapsed.set_xlabel('Window (samples)')\n",
    "ax_collapsed.set_yticks(np.arange(len(offsets)))\n",
    "ax_collapsed.set_yticklabels(offsets, rotation=45, fontsize=8)\n",
    "ax_collapsed.set_xticks(np.arange(len(windows_for_collapsed_cube)))\n",
    "ax_collapsed.set_xticklabels(windows_for_collapsed_cube, fontsize=8)\n",
    "# add color bar to the collapsed plot\n",
    "cbar_collapsed = fig_collapsed.colorbar(plt.cm.ScalarMappable(norm=norm_collapsed, cmap=cmap_collapsed), ax=ax_collapsed, fraction=0.032, pad=0.04)\n",
    "cbar_collapsed.ax.tick_params(labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sim_source:\n",
    "    # save the collapsed figure (png and PDF) with the simulated source flux\n",
    "    fig_collapsed.savefig(f'analysis_pairs/Figures/collapsed_fraction_detected_pulses_cube_windows_offsets_{sim_source_flux:.2f}mcrab.png', dpi=300, bbox_inches='tight')\n",
    "    #fig_collapsed.savefig(f'analysis_pairs/Figures/collapsed_fraction_detected_pulses_cube_windows_offsets_{sim_source_flux:.2f}mcrab.pdf', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    # save the collapsed figure (png and PDF)\n",
    "    fig_collapsed.savefig(f'analysis_pairs/Figures/collapsed_fraction_detected_pulses_cube_windows_offsets.png', dpi=300, bbox_inches='tight')\n",
    "    #fig_collapsed.savefig(f'analysis_pairs/Figures/collapsed_fraction_detected_pulses_cube_windows_offsets.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503b334",
   "metadata": {},
   "source": [
    "### Some DUMB tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aux.is_notebook:\n",
    "    win= 6\n",
    "    off = 0\n",
    "    # print values of cube_collapsed for window=win, offset=off and separation=sep_for_plot_mosaic\n",
    "    isep = np.where(np.array(relevant_separations) == sep_for_plot_mosaic)[0][0]  # find the index of separation=sep_for_plot_mosaic\n",
    "    iw = windows_for_collapsed_cube.index(win)  # find the index of window=win\n",
    "    io = np.where(np.array(offsets) == off)[0][0]  # find the index of offset=off  \n",
    "\n",
    "    print(f\"cube_collapsed[io, iw]: {cube_collapsed[io, iw]:.2f}\")\n",
    "    print(f\"cube_e1e2collapsed[isep, io, iw]: {cube_e1e2collapsed[isep, io, iw]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aux.is_notebook:\n",
    "    ########### TESTING PART ###########\n",
    "    # This part is for testing purposes only, to read the data from a pickle file and print\n",
    "    # the filtered data and an example row with specific values.\n",
    "    # It should not be part of the main code execution.\n",
    "\n",
    "    #read the data for window=20 and offset=0\n",
    "    win = 0\n",
    "    off = 0\n",
    "    e1 = 0.2\n",
    "    e2 = 0.2\n",
    "    pickle_file = f'{pickles_dir}/detectedFakes_win{win}_off{off}.pkl'\n",
    "    # read the data from the pickle file\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        if win == 0 and off == 0:\n",
    "            # for window=0 and offset=0, we have a different data structure\n",
    "            data_table = table.Table(rows=data, names=('separation', 'energy1', 'energy2', 'samplesDown', 'samplesUp', 'threshold', 'ndetected', 'nfake'))\n",
    "            data_filtered = data_table[(data_table['threshold'] == th) & (data_table['samplesUp'] == sUp) & (data_table['samplesDown'] == sDown)]\n",
    "        else:\n",
    "            # for other windows and offsets, we have a different data structure\n",
    "            data_table = table.Table(rows=data, names=('separation', 'energy1', 'energy2', 'window', 'offset', 'ndetected', 'nfake')) \n",
    "            data_filtered = data_table.copy()\n",
    "    #print(f\"Filtered data: {data_filtered}\")\n",
    "    # print row with seaparation=20, energy1=0.2, energy2=0.5\n",
    "    example_row = data_filtered[(data_filtered['separation'] == sep_for_plot_mosaic) & (data_filtered['energy1'] == e1) & (data_filtered['energy2'] == e2)]\n",
    "    print(f\"Example row:\\n {example_row}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc951d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSFCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
